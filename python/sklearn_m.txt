import sklearn

классический набор данных в машинном обучении и статистике
	from sklearn.datasets import load_iris
	dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 
		'filename'])  # объект похожий на словарь
	DESCR - описание набора данных
	target_names - массив строк, содержащий сорта светов, соответствуют меткам
	feature_names – список строк с описанием каждого признака, фичи
	filename - путь к файлу
	data - массив[][] NumPy который содержит количественные измерения фич
	target - массив, сорта уже измеренных цветов, метки.

Обучающиe данныe (training data/training set) - используется для построения 
	модели машинного обучения
	X_train - обучающие исходные данные
	y_train - обучающие ответы/метки данных
Тестовые данные/контрольный набор (test data/ test set/ hold out set) - 
	используются для оценки качества модели
	X_test - контрольные исходные данные
	y_test - ответы/метки контрольных данных
			
train_test_split() перемешивает набор данных и разбивает его на две 
	части. Эта функция отбирает в обучающий набор 75% строк данных с 
	соответствующими метками. Оставшиеся 25% данных с метками 
	объявляются тестовым набором.
	X_train, X_test, y_train, y_test = train_test_split(
		iris_dataset['data'], iris_dataset['target'], random_state=0)
					
scatter_matrix сравнивает все признаки между друг другом, и строит 
	матрицу из рисунков, по диагонали гистограмма этого признака.
	iris_dataframe = pd.DataFrame(X_train, создаем dataframe
		columns=iris_dataset.feature_names)
	scatter_matrix(iris_dataframe, c=y_train, figsize=(10, 10), 
		marker='o', hist_kwds={'bins': 20}, s=60, alpha=.8)

Все модели МО реализованы в классах Estimator.

Переобучение - слишком сложная модель.
Недообучение - слишком простая модель
Обобщающая способность - способность модели принимать верные решения.



Машинное обучение с учителем.
	Это когда на известных данных вопрос-ответ строится модель, затем тестируем и 
		прогнозируем ответ для новых данных.

	Классификация
		цель спрогнозировать метку класса из заранее определенных меток - 
			ответов. Существует бинарная классификация и мульти, 2 варианта или 
			больше. Например сорт цветка.
		
		Метод k ближайших соседей
			Суть в запоминании тренеровочного набора, затем получив новые 
				данные, строится точка на основе признаков, находится соседняя 
				ближайшая точка и присваивается ее метка нашему тестовому набору.
			sklearn.neighbors.KNeighborsClassifier - метод ближайших соседей  
			knn = KNeighborsClassifier(n_neighbors=1) кол-во соседей
			knn.fit(X_train, y_train) построение модели
			knn.score(X_test, y_test) проверка на точность на тест наборе
			knn.predict(np.array([[1,2,3,4])) а это новые данные, двумерный 
				массив, можно сразу много данных knn.predict(X_test)
			Увеличение числа соседей приводит к сглаживанию границы принятия 
				решений
			Граница принятия решений - плоскость разбитая на части,
				в соответствии с классами.
			Чем больше соседей тем ниже сложность модели, граница принятия 
				решений сглаживается.

			
	Регрессия
		цель спрогнозировать непрерывное число с плав точкой. Например годовой 
			доход.
		Включение производных признаков называется конструированием признаков
		Метод ближайших соседей для регрессии
			KNeighborsRegressor
			
	Линейные модели. 
    Используют для прогноза линейную функцию входных признаков.
    Линейные модели для регрессии
        y = w[0]*x[0] + w[1]*x[1] + w[1]*x[1] + … + w[p]*x[p] + b
            x[0] … x[p]  - признаки в количестве p-1
            w, b  - параметры модели, оцениваемые в ходе обуч
            y  - прогноз выдаваемый моделью

            x[0]*w[0] - наклон линии
            w -  веса/коэффициент - tg угла наклона
            b - сдвиг/константа по оси ординат
	
		Линейная регрессия или обычный метод наименьших квадратов
			Находит параметры w и b, которые минимизируют среднеквадратическую ошибку (mean squared error) между спрогнозированными и фактическими ответами у в обучающем наборе. Среднеквадратичная ошибка равна сумме квадратов разностей между спрогнозированными и фактическими значениями
			from sklearn.linear_model import LinearRegression
			lr = LinearRegression().fit(X_train, y_train)
			lr.coef_  все веса
			lr.intercept_ сдвиг

		Гребневая регрессия, Ridge
			Схожа с методом наименьших квадратов, но параметры w b подгоняются с ограничениями. Регулиризация - ограничение модели для предотвращения переобучения.
			from sklearn.linear_model import Ridge
			ridge = Ridge(alpha=1).fit(X_train, y_train)
				alpha - коэф качества работы, при равном нулю - Лин рег.
				
	Линейные модели для классификации
		Бинарная классификация
			y = w[0]*x[0] + w[1]*x[1] + w[1]*x[1] + … + w[p]*x[p] + b > 0
			Если > 0 то класс первый, если меньше второй класс.




	
